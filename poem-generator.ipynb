{"cells":[{"cell_type":"markdown","metadata":{},"source":["Library imports"]},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-26T23:56:56.838254Z","iopub.status.busy":"2024-02-26T23:56:56.837923Z","iopub.status.idle":"2024-02-26T23:56:56.844401Z","shell.execute_reply":"2024-02-26T23:56:56.841895Z","shell.execute_reply.started":"2024-02-26T23:56:56.838230Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import regex as re\n","import numpy as np\n","import pandas as pd\n","import os"]},{"cell_type":"markdown","metadata":{},"source":["Class Definitions"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:56:57.037875Z","iopub.status.busy":"2024-02-26T23:56:57.037501Z","iopub.status.idle":"2024-02-26T23:56:57.044181Z","shell.execute_reply":"2024-02-26T23:56:57.043076Z","shell.execute_reply.started":"2024-02-26T23:56:57.037844Z"},"trusted":true},"outputs":[],"source":["class FeedForward(nn.Module):\n","    \n","    def __init__(self):\n","        super(FeedForward, self).__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_hidden, 2 * n_hidden),\n","            nn.ReLU(),\n","            nn.Linear(2 * n_hidden, n_hidden),\n","            nn.Dropout(dropout)\n","        )\n","        \n","    def forward(self, x):\n","        return self.net(x)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:56:57.185625Z","iopub.status.busy":"2024-02-26T23:56:57.184701Z","iopub.status.idle":"2024-02-26T23:56:57.193845Z","shell.execute_reply":"2024-02-26T23:56:57.192924Z","shell.execute_reply.started":"2024-02-26T23:56:57.185583Z"},"trusted":true},"outputs":[],"source":["class Head(nn.Module):\n","    \n","    def __init__(self):\n","        super(Head, self).__init__()\n","        self.q = nn.Linear(n_hidden, head_size)\n","        self.k = nn.Linear(n_hidden, head_size)\n","        self.v = nn.Linear(n_hidden, head_size)\n","        \n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, x):\n","        B, T, C = x.shape\n","        \n","        q = self.q(x)\n","        k = self.k(x)\n","        v = self.v(x)\n","        \n","        out = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n","        out = out.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n","        out = F.softmax(out, dim = -1)\n","        out = self.dropout(out)\n","        out = out @ v\n","        \n","        return out"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:56:57.329991Z","iopub.status.busy":"2024-02-26T23:56:57.329132Z","iopub.status.idle":"2024-02-26T23:56:57.336414Z","shell.execute_reply":"2024-02-26T23:56:57.335391Z","shell.execute_reply.started":"2024-02-26T23:56:57.329957Z"},"trusted":true},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    \n","    def __init__(self):\n","        super(MultiHeadAttention, self).__init__()\n","        self.heads = nn.ModuleList([Head() for _ in range(n_heads)])\n","        self.proj = nn.Linear(head_size * n_heads, n_hidden)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim = -1)\n","        out = self.dropout(self.proj(out))\n","        return out"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:56:57.466559Z","iopub.status.busy":"2024-02-26T23:56:57.466265Z","iopub.status.idle":"2024-02-26T23:56:57.472431Z","shell.execute_reply":"2024-02-26T23:56:57.471532Z","shell.execute_reply.started":"2024-02-26T23:56:57.466536Z"},"trusted":true},"outputs":[],"source":["class Block(nn.Module):\n","    \n","    def __init__(self):\n","        super(Block, self).__init__()\n","        self.sa = MultiHeadAttention()\n","        self.ffwd = FeedForward()\n","        self.ln1 = nn.LayerNorm(n_hidden)\n","        self.ln2 = nn.LayerNorm(n_hidden)\n","        \n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:56:57.600400Z","iopub.status.busy":"2024-02-26T23:56:57.599905Z","iopub.status.idle":"2024-02-26T23:56:57.613533Z","shell.execute_reply":"2024-02-26T23:56:57.612552Z","shell.execute_reply.started":"2024-02-26T23:56:57.600376Z"},"trusted":true},"outputs":[],"source":["class GPTLanguageModel(nn.Module):\n","    \n","    def __init__(self):\n","        super(GPTLanguageModel, self).__init__()\n","        self.tokens = nn.Embedding(tokenizer.vocab_size, n_hidden)\n","        self.positions = nn.Embedding(block_size, n_hidden)\n","        self.blocks = nn.Sequential(*[Block() for _ in range(n_layers)])\n","        self.ln_f = nn.LayerNorm(n_hidden)\n","        self.lm_head = nn.Linear(n_hidden, tokenizer.vocab_size)\n","        \n","        self.apply(self._init_weights)\n","        \n","    def _init_weights(self, module):\n","        \n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n","            \n","    def forward(self, idx, targets = None):\n","        B, T = idx.shape\n","        x = self.tokens(idx) + self.positions(torch.arange(T, device = device))\n","        x = self.blocks(x)\n","        x = self.ln_f(x)\n","        logits = self.lm_head(x)\n","        \n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B * T, C)\n","            targets = targets.view(B * T)\n","            loss = F.cross_entropy(logits, targets)\n","            \n","        return logits,loss\n","    \n","    def generate(self, idx):\n","        model.eval()\n","        \n","        while True:\n","            idx_cond = idx[:, -block_size:]\n","            logits, loss = self(idx_cond)\n","            logits = logits[:, -1, :]\n","            probs = F.softmax(logits, dim = -1)\n","            idx_next = torch.multinomial(probs, num_samples = 1)\n","            \n","            if idx_next == tokenizer.special_tokens[\"</Poem>\"]:\n","                break\n","            \n","            idx = torch.cat((idx, idx_next), dim = 1)\n","            \n","        model.train()\n","        return idx"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:56:57.940636Z","iopub.status.busy":"2024-02-26T23:56:57.940242Z","iopub.status.idle":"2024-02-26T23:56:57.961649Z","shell.execute_reply":"2024-02-26T23:56:57.960721Z","shell.execute_reply.started":"2024-02-26T23:56:57.940604Z"},"trusted":true},"outputs":[],"source":["class RegexTokenizer():\n","    \n","    \n","    def __init__(self, pattern = None):\n","        self.merges = {}\n","        self.vocab = {idx: bytes([idx]) for idx in range(256)}\n","        self.vocab_size = 256\n","        self.pattern = pattern\n","        self.compiled_pattern = re.compile(self.pattern)\n","        self.special_tokens = {}\n","        self.inverse_special_tokens = {}\n","        \n","        \n","    def register_special_tokens(self, special_tokens):\n","        \n","        for token in special_tokens:\n","            print(token)\n","            self.special_tokens[token] = self.vocab_size\n","            self.vocab[self.vocab_size] = token.encode(\"utf-8\")\n","            self.vocab_size += 1\n","            \n","        self.inverse_special_tokens = {v: k for k, v in self.special_tokens.items()}\n","        \n","        \n","    def get_stats(self, ids, counts = None):\n","        counts = {} if counts is None else counts\n","        for pair in zip(ids, ids[1:]):\n","            counts[pair] = counts.get(pair, 0) + 1\n","        return counts\n","    \n","    \n","    def merge(self, ids, pair, idx):\n","        \n","        newids = []\n","        i = 0\n","        \n","        while i < len(ids):\n","            if i < len(ids) - 1 and pair[0] == ids[i] and pair[1] == ids[i + 1]:\n","                newids.append(idx)\n","                i += 2\n","            else:\n","                newids.append(ids[i])\n","                i += 1\n","        \n","        return newids\n","    \n","    \n","    def train(self, text, vocab_size, verbose = False):\n","        \n","        chunks = re.findall(self.compiled_pattern, text)\n","        \n","        ids = [list(chunk.encode(\"utf-8\")) for chunk in chunks]\n","        \n","        while self.vocab_size < vocab_size:\n","            \n","            stats = {}\n","            \n","            for chunk in ids:\n","                self.get_stats(chunk, stats)\n","                \n","            pair = max(stats, key = stats.get)\n","            \n","            ids = [self.merge(chunk, pair, self.vocab_size) for chunk in ids]\n","            \n","            self.merges[pair] = self.vocab_size\n","            self.vocab[self.vocab_size] = self.vocab[pair[0]] + self.vocab[pair[1]]\n","            \n","            if verbose:\n","                print(f\"merging {self.vocab[pair[0]], self.vocab[pair[1]]} -> {self.vocab_size}\")\n","                \n","            self.vocab_size += 1\n","            \n","            \n","    def decode(self, ids):\n","        \n","        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n","        text = text_bytes.decode(\"utf-8\", errors = \"replace\")\n","        return text\n","    \n","    \n","    def _encode_chunk(self, chunk_bytes):\n","        ids = list(chunk_bytes)\n","        \n","        while len(ids) >= 2:\n","            stats = self.get_stats(ids)\n","            pair = min(stats, key = lambda p: self.merges.get(p, float(\"inf\")))\n","            \n","            if pair not in self.merges:\n","                break\n","            ids = self.merge(ids, pair, self.merges[pair])\n","            \n","        return ids\n","    \n","    def encode(self, text):\n","        chunks = re.findall(self.compiled_pattern, text)\n","        \n","        ids = []\n","        for chunk in chunks:\n","            chunk_encoded = self._encode_chunk(chunk.encode(\"utf-8\"))\n","            ids.extend(chunk_encoded)\n","            \n","        return ids"]},{"cell_type":"markdown","metadata":{},"source":["Setting up data and tokenizer"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:56:58.167572Z","iopub.status.busy":"2024-02-26T23:56:58.166709Z","iopub.status.idle":"2024-02-26T23:56:58.508493Z","shell.execute_reply":"2024-02-26T23:56:58.507502Z","shell.execute_reply.started":"2024-02-26T23:56:58.167540Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv(\"/kaggle/input/poetry-foundation-poems/PoetryFoundationData.csv\")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:56:58.587210Z","iopub.status.busy":"2024-02-26T23:56:58.586550Z","iopub.status.idle":"2024-02-26T23:56:59.111696Z","shell.execute_reply":"2024-02-26T23:56:59.110925Z","shell.execute_reply.started":"2024-02-26T23:56:58.587176Z"},"trusted":true},"outputs":[],"source":["text = \"\"\n","for i in range(len(data)):\n","    text += data.Title[i]\n","    text += data.Poem[i]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer_text = text[48 * len(text) // 100:52 * len(text) // 100]\n","tokenizer = RegexTokenizer(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\")\n","tokenizer.train(tokenizer_text, 1000, verbose = True)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T00:05:49.006077Z","iopub.status.busy":"2024-02-27T00:05:49.005715Z","iopub.status.idle":"2024-02-27T00:05:49.011254Z","shell.execute_reply":"2024-02-27T00:05:49.010381Z","shell.execute_reply.started":"2024-02-27T00:05:49.006049Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<Title>\n","</Title><Poem>\n","</Poem>\n"]}],"source":["# RUN ONCE\n","tokenizer.register_special_tokens([\"<Title>\", \"</Title><Poem>\", \"</Poem>\"])"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T00:06:25.943468Z","iopub.status.busy":"2024-02-27T00:06:25.942665Z","iopub.status.idle":"2024-02-27T00:06:25.949227Z","shell.execute_reply":"2024-02-27T00:06:25.948426Z","shell.execute_reply.started":"2024-02-27T00:06:25.943439Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(1003, b'</Poem>')"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.vocab_size, tokenizer.vocab[1002]"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T00:08:38.349651Z","iopub.status.busy":"2024-02-27T00:08:38.349078Z","iopub.status.idle":"2024-02-27T00:08:38.357304Z","shell.execute_reply":"2024-02-27T00:08:38.356466Z","shell.execute_reply.started":"2024-02-27T00:08:38.349620Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'<Title>/What/ is/ that/ me/l/od/y/!/</Title><Poem>'"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["# visualization of encoding/decoding\n","ids = [1000]\n","ids.extend(tokenizer.encode(\"What is that melody!\"))\n","ids.append(1001)\n","decoded = []\n","for i in range(len(ids)):\n","    ids[i] = tokenizer.vocab[ids[i]].decode(\"utf-8\")\n","\"/\".join(ids)"]},{"cell_type":"markdown","metadata":{},"source":["Model hyperparameters"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T00:13:31.984538Z","iopub.status.busy":"2024-02-27T00:13:31.984194Z","iopub.status.idle":"2024-02-27T00:13:31.992083Z","shell.execute_reply":"2024-02-27T00:13:31.991094Z","shell.execute_reply.started":"2024-02-27T00:13:31.984510Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_hidden = 1024\n","block_size = 128\n","n_layers = 6\n","n_heads = 8\n","head_size = 128\n","dropout = 0.1\n","learning_rate = 3e-4\n","total_steps = 5000\n","batch_size = 64\n","\n","device"]},{"cell_type":"markdown","metadata":{},"source":["Helper functions"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T00:09:17.901204Z","iopub.status.busy":"2024-02-27T00:09:17.900560Z","iopub.status.idle":"2024-02-27T00:09:17.909550Z","shell.execute_reply":"2024-02-27T00:09:17.908786Z","shell.execute_reply.started":"2024-02-27T00:09:17.901172Z"},"trusted":true},"outputs":[],"source":["def build_dataset():\n","    text = []\n","    for i in range(len(data)):\n","        text.append(tokenizer.special_tokens[\"<Title>\"])\n","        text.extend(tokenizer.encode(data.Title[i]))\n","        text.append(tokenizer.special_tokens[\"</Title><Poem>\"])\n","        text.extend(tokenizer.encode(data.Poem[i]))\n","        text.append(tokenizer.special_tokens[\"</Poem>\"])\n","        \n","    train_text = text[:8 * len(text) // 10]\n","    test_text = text[8 * len(text) // 10:]\n","    \n","    return train_text, test_text"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T00:09:18.494171Z","iopub.status.busy":"2024-02-27T00:09:18.493846Z","iopub.status.idle":"2024-02-27T00:09:18.500746Z","shell.execute_reply":"2024-02-27T00:09:18.499719Z","shell.execute_reply.started":"2024-02-27T00:09:18.494145Z"},"trusted":true},"outputs":[],"source":["def get_batch(split):\n","    X, Y = [], []\n","    \n","    split_text = {\n","        \"train\": train_text,\n","        \"test\": test_text,\n","    }[split]\n","    \n","    ix = torch.randint(0, len(split_text) - 128 - 1, (batch_size,))\n","    \n","    for idx in ix:\n","        X.append(split_text[idx: idx + 128])\n","        Y.append(split_text[idx + 1: idx + 128 + 1])\n","    \n","    return torch.tensor(np.array(X)).to(device), torch.tensor(np.array(Y)).to(device)"]},{"cell_type":"code","execution_count":194,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T01:19:26.117776Z","iopub.status.busy":"2024-02-27T01:19:26.117026Z","iopub.status.idle":"2024-02-27T01:19:26.124175Z","shell.execute_reply":"2024-02-27T01:19:26.123310Z","shell.execute_reply.started":"2024-02-27T01:19:26.117744Z"},"trusted":true},"outputs":[],"source":["def train():\n","    model.train()\n","    \n","    optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate / 3) # take off / 3 for fresh models, i am only doing this for finer training/tuning\n","    \n","    for i in range(total_steps):\n","        \n","        xb, yb = get_batch(\"train\")\n","        logits, loss = model(xb, yb)\n","        optimizer.zero_grad(set_to_none = True)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if (i + 1) % 10 == 0:\n","            print(\"Step[{}/{}], Loss: {:.4f}\".format(i + 1, total_steps, loss.item()))"]},{"cell_type":"code","execution_count":191,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T01:15:40.558765Z","iopub.status.busy":"2024-02-27T01:15:40.558106Z","iopub.status.idle":"2024-02-27T01:15:40.566016Z","shell.execute_reply":"2024-02-27T01:15:40.565084Z","shell.execute_reply.started":"2024-02-27T01:15:40.558732Z"},"trusted":true},"outputs":[],"source":["def generatePoem(context):\n","    \n","    model.eval()\n","    # all poems start with all this extra formatting so this is needed in order for the title section to be perceived by the model as being in distribution and not an anomaly\n","    title = \"\\r\\r\\n                    \" + context + \"\\r\\r\\n                \"\n","    model_context = [tokenizer.special_tokens[\"<Title>\"],]\n","    model_context.extend(tokenizer.encode(title))\n","    model_context.append(tokenizer.special_tokens[\"</Title><Poem>\"])\n","    context_encoded = torch.tensor(model_context).view(1, -1).to(device)\n","    response = np.array(model.generate(context_encoded).to(\"cpu\"))[0]\n","    \n","    response_length = len(response)\n","    i = 0\n","    while i < response_length:\n","        # a little bit of surgery to cut out all special tokens (ids >= 1000) in the response\n","        if response[i] >= 1000:\n","            response = np.delete(response, i)\n","            response_length -= 1\n","        i += 1\n","    \n","    print(tokenizer.decode(response))"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T00:09:22.261331Z","iopub.status.busy":"2024-02-27T00:09:22.260970Z","iopub.status.idle":"2024-02-27T00:09:22.267308Z","shell.execute_reply":"2024-02-27T00:09:22.266381Z","shell.execute_reply.started":"2024-02-27T00:09:22.261290Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'<Title>': 1000, '</Title><Poem>': 1001, '</Poem>': 1002}"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.special_tokens"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T00:09:24.985264Z","iopub.status.busy":"2024-02-27T00:09:24.984442Z","iopub.status.idle":"2024-02-27T00:11:05.997782Z","shell.execute_reply":"2024-02-27T00:11:05.996665Z","shell.execute_reply.started":"2024-02-27T00:09:24.985231Z"},"trusted":true},"outputs":[],"source":["train_text, test_text = build_dataset()"]},{"cell_type":"markdown","metadata":{},"source":["Initializing and training model"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T00:24:04.625731Z","iopub.status.busy":"2024-02-27T00:24:04.625341Z","iopub.status.idle":"2024-02-27T00:24:05.663328Z","shell.execute_reply":"2024-02-27T00:24:05.662352Z","shell.execute_reply.started":"2024-02-27T00:24:04.625700Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["52.587499 million parameters\n"]}],"source":["model = GPTLanguageModel().to(device)\n","print(sum(p.numel() for p in model.parameters())/1e6, \"million parameters\")"]},{"cell_type":"code","execution_count":195,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T01:19:30.984772Z","iopub.status.busy":"2024-02-27T01:19:30.984156Z","iopub.status.idle":"2024-02-27T01:57:04.169932Z","shell.execute_reply":"2024-02-27T01:57:04.168945Z","shell.execute_reply.started":"2024-02-27T01:19:30.984739Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Step[10/5000], Loss: 3.0445\n","Step[20/5000], Loss: 3.0853\n","Step[30/5000], Loss: 3.0711\n","Step[40/5000], Loss: 2.9821\n","Step[50/5000], Loss: 3.0194\n","Step[60/5000], Loss: 3.0725\n","Step[70/5000], Loss: 3.0739\n","Step[80/5000], Loss: 3.0743\n","Step[90/5000], Loss: 3.0958\n","Step[100/5000], Loss: 3.0325\n","Step[110/5000], Loss: 2.9998\n","Step[120/5000], Loss: 3.0752\n","Step[130/5000], Loss: 2.9962\n","Step[140/5000], Loss: 3.0899\n","Step[150/5000], Loss: 3.0040\n","Step[160/5000], Loss: 3.0007\n","Step[170/5000], Loss: 3.0323\n","Step[180/5000], Loss: 2.9382\n","Step[190/5000], Loss: 3.0079\n","Step[200/5000], Loss: 2.9813\n","Step[210/5000], Loss: 2.9960\n","Step[220/5000], Loss: 3.0758\n","Step[230/5000], Loss: 2.9997\n","Step[240/5000], Loss: 3.1051\n","Step[250/5000], Loss: 3.0757\n","Step[260/5000], Loss: 3.0972\n","Step[270/5000], Loss: 3.0883\n","Step[280/5000], Loss: 3.0379\n","Step[290/5000], Loss: 3.0271\n","Step[300/5000], Loss: 3.0428\n","Step[310/5000], Loss: 3.0532\n","Step[320/5000], Loss: 2.9830\n","Step[330/5000], Loss: 3.0072\n","Step[340/5000], Loss: 3.0631\n","Step[350/5000], Loss: 2.9883\n","Step[360/5000], Loss: 2.9796\n","Step[370/5000], Loss: 2.9712\n","Step[380/5000], Loss: 3.0400\n","Step[390/5000], Loss: 3.0333\n","Step[400/5000], Loss: 3.0427\n","Step[410/5000], Loss: 3.0262\n","Step[420/5000], Loss: 3.0481\n","Step[430/5000], Loss: 2.9732\n","Step[440/5000], Loss: 3.0359\n","Step[450/5000], Loss: 2.9890\n","Step[460/5000], Loss: 3.0057\n","Step[470/5000], Loss: 3.0592\n","Step[480/5000], Loss: 3.0292\n","Step[490/5000], Loss: 3.0440\n","Step[500/5000], Loss: 3.0508\n","Step[510/5000], Loss: 3.0282\n","Step[520/5000], Loss: 2.9684\n","Step[530/5000], Loss: 3.0090\n","Step[540/5000], Loss: 2.9989\n","Step[550/5000], Loss: 3.0255\n","Step[560/5000], Loss: 2.9911\n","Step[570/5000], Loss: 3.0223\n","Step[580/5000], Loss: 2.9581\n","Step[590/5000], Loss: 3.0356\n","Step[600/5000], Loss: 3.0105\n","Step[610/5000], Loss: 3.0497\n","Step[620/5000], Loss: 2.9964\n","Step[630/5000], Loss: 2.9957\n","Step[640/5000], Loss: 2.9585\n","Step[650/5000], Loss: 3.0516\n","Step[660/5000], Loss: 3.0497\n","Step[670/5000], Loss: 2.9840\n","Step[680/5000], Loss: 2.9944\n","Step[690/5000], Loss: 3.0116\n","Step[700/5000], Loss: 3.0075\n","Step[710/5000], Loss: 3.0311\n","Step[720/5000], Loss: 3.0013\n","Step[730/5000], Loss: 3.0622\n","Step[740/5000], Loss: 3.0402\n","Step[750/5000], Loss: 2.9264\n","Step[760/5000], Loss: 2.9365\n","Step[770/5000], Loss: 2.9864\n","Step[780/5000], Loss: 3.0400\n","Step[790/5000], Loss: 3.0046\n","Step[800/5000], Loss: 2.9590\n","Step[810/5000], Loss: 2.9532\n","Step[820/5000], Loss: 3.0004\n","Step[830/5000], Loss: 2.9549\n","Step[840/5000], Loss: 2.9677\n","Step[850/5000], Loss: 2.9540\n","Step[860/5000], Loss: 3.0271\n","Step[870/5000], Loss: 3.0190\n","Step[880/5000], Loss: 3.0134\n","Step[890/5000], Loss: 3.0276\n","Step[900/5000], Loss: 2.9855\n","Step[910/5000], Loss: 2.9613\n","Step[920/5000], Loss: 3.0474\n","Step[930/5000], Loss: 3.0061\n","Step[940/5000], Loss: 2.9406\n","Step[950/5000], Loss: 3.0038\n","Step[960/5000], Loss: 2.9980\n","Step[970/5000], Loss: 2.9506\n","Step[980/5000], Loss: 3.0170\n","Step[990/5000], Loss: 3.0543\n","Step[1000/5000], Loss: 3.0150\n","Step[1010/5000], Loss: 2.9805\n","Step[1020/5000], Loss: 2.8935\n","Step[1030/5000], Loss: 3.0248\n","Step[1040/5000], Loss: 2.9439\n","Step[1050/5000], Loss: 2.9625\n","Step[1060/5000], Loss: 2.9509\n","Step[1070/5000], Loss: 2.9821\n","Step[1080/5000], Loss: 3.0218\n","Step[1090/5000], Loss: 2.9290\n","Step[1100/5000], Loss: 2.9522\n","Step[1110/5000], Loss: 2.9699\n","Step[1120/5000], Loss: 2.9904\n","Step[1130/5000], Loss: 2.9426\n","Step[1140/5000], Loss: 2.9533\n","Step[1150/5000], Loss: 3.0221\n","Step[1160/5000], Loss: 2.9270\n","Step[1170/5000], Loss: 3.0193\n","Step[1180/5000], Loss: 2.9805\n","Step[1190/5000], Loss: 2.8991\n","Step[1200/5000], Loss: 2.9518\n","Step[1210/5000], Loss: 3.0007\n","Step[1220/5000], Loss: 2.9409\n","Step[1230/5000], Loss: 3.0294\n","Step[1240/5000], Loss: 2.8884\n","Step[1250/5000], Loss: 3.0475\n","Step[1260/5000], Loss: 2.9967\n","Step[1270/5000], Loss: 2.9285\n","Step[1280/5000], Loss: 2.9933\n","Step[1290/5000], Loss: 2.9612\n","Step[1300/5000], Loss: 2.9600\n","Step[1310/5000], Loss: 3.0066\n","Step[1320/5000], Loss: 2.9861\n","Step[1330/5000], Loss: 3.0327\n","Step[1340/5000], Loss: 2.9961\n","Step[1350/5000], Loss: 2.9772\n","Step[1360/5000], Loss: 2.9037\n","Step[1370/5000], Loss: 2.9449\n","Step[1380/5000], Loss: 2.9794\n","Step[1390/5000], Loss: 2.9519\n","Step[1400/5000], Loss: 3.0378\n","Step[1410/5000], Loss: 2.9171\n","Step[1420/5000], Loss: 2.9228\n","Step[1430/5000], Loss: 2.9514\n","Step[1440/5000], Loss: 2.9325\n","Step[1450/5000], Loss: 2.8307\n","Step[1460/5000], Loss: 2.9560\n","Step[1470/5000], Loss: 2.9751\n","Step[1480/5000], Loss: 2.9953\n","Step[1490/5000], Loss: 2.9797\n","Step[1500/5000], Loss: 2.9648\n","Step[1510/5000], Loss: 2.9937\n","Step[1520/5000], Loss: 2.9500\n","Step[1530/5000], Loss: 2.8482\n","Step[1540/5000], Loss: 2.9987\n","Step[1550/5000], Loss: 2.9919\n","Step[1560/5000], Loss: 2.9111\n","Step[1570/5000], Loss: 2.9396\n","Step[1580/5000], Loss: 2.9377\n","Step[1590/5000], Loss: 2.9830\n","Step[1600/5000], Loss: 2.9614\n","Step[1610/5000], Loss: 2.9303\n","Step[1620/5000], Loss: 2.9095\n","Step[1630/5000], Loss: 2.9514\n","Step[1640/5000], Loss: 2.9690\n","Step[1650/5000], Loss: 2.9479\n","Step[1660/5000], Loss: 2.9577\n","Step[1670/5000], Loss: 2.9305\n","Step[1680/5000], Loss: 2.9709\n","Step[1690/5000], Loss: 2.9484\n","Step[1700/5000], Loss: 2.9905\n","Step[1710/5000], Loss: 2.9511\n","Step[1720/5000], Loss: 2.9461\n","Step[1730/5000], Loss: 2.9551\n","Step[1740/5000], Loss: 2.9816\n","Step[1750/5000], Loss: 2.9228\n","Step[1760/5000], Loss: 2.9263\n","Step[1770/5000], Loss: 2.9333\n","Step[1780/5000], Loss: 2.8887\n","Step[1790/5000], Loss: 2.9686\n","Step[1800/5000], Loss: 2.9169\n","Step[1810/5000], Loss: 2.9321\n","Step[1820/5000], Loss: 2.9488\n","Step[1830/5000], Loss: 2.9461\n","Step[1840/5000], Loss: 3.0063\n","Step[1850/5000], Loss: 2.9176\n","Step[1860/5000], Loss: 2.8984\n","Step[1870/5000], Loss: 2.9214\n","Step[1880/5000], Loss: 2.9286\n","Step[1890/5000], Loss: 2.8851\n","Step[1900/5000], Loss: 2.9638\n","Step[1910/5000], Loss: 3.0263\n","Step[1920/5000], Loss: 2.9508\n","Step[1930/5000], Loss: 2.8907\n","Step[1940/5000], Loss: 2.9826\n","Step[1950/5000], Loss: 2.9287\n","Step[1960/5000], Loss: 2.9167\n","Step[1970/5000], Loss: 2.9045\n","Step[1980/5000], Loss: 2.9284\n","Step[1990/5000], Loss: 2.9107\n","Step[2000/5000], Loss: 2.9768\n","Step[2010/5000], Loss: 2.9091\n","Step[2020/5000], Loss: 2.9158\n","Step[2030/5000], Loss: 2.9362\n","Step[2040/5000], Loss: 2.9422\n","Step[2050/5000], Loss: 2.9022\n","Step[2060/5000], Loss: 2.8873\n","Step[2070/5000], Loss: 2.9329\n","Step[2080/5000], Loss: 2.8993\n","Step[2090/5000], Loss: 2.9151\n","Step[2100/5000], Loss: 2.8936\n","Step[2110/5000], Loss: 2.9549\n","Step[2120/5000], Loss: 2.9202\n","Step[2130/5000], Loss: 2.9950\n","Step[2140/5000], Loss: 2.9537\n","Step[2150/5000], Loss: 2.9177\n","Step[2160/5000], Loss: 2.9374\n","Step[2170/5000], Loss: 3.0106\n","Step[2180/5000], Loss: 2.9224\n","Step[2190/5000], Loss: 2.9006\n","Step[2200/5000], Loss: 2.9123\n","Step[2210/5000], Loss: 2.9518\n","Step[2220/5000], Loss: 2.9571\n","Step[2230/5000], Loss: 2.9112\n","Step[2240/5000], Loss: 2.9293\n","Step[2250/5000], Loss: 2.8445\n","Step[2260/5000], Loss: 2.8204\n","Step[2270/5000], Loss: 2.9291\n","Step[2280/5000], Loss: 2.9371\n","Step[2290/5000], Loss: 2.9083\n","Step[2300/5000], Loss: 2.9483\n","Step[2310/5000], Loss: 2.9259\n","Step[2320/5000], Loss: 2.9339\n","Step[2330/5000], Loss: 2.9574\n","Step[2340/5000], Loss: 2.9953\n","Step[2350/5000], Loss: 2.9376\n","Step[2360/5000], Loss: 2.9330\n","Step[2370/5000], Loss: 2.9911\n","Step[2380/5000], Loss: 2.9053\n","Step[2390/5000], Loss: 2.9019\n","Step[2400/5000], Loss: 2.8524\n","Step[2410/5000], Loss: 2.8891\n","Step[2420/5000], Loss: 2.8775\n","Step[2430/5000], Loss: 2.8248\n","Step[2440/5000], Loss: 2.9429\n","Step[2450/5000], Loss: 3.0118\n","Step[2460/5000], Loss: 2.9343\n","Step[2470/5000], Loss: 2.9281\n","Step[2480/5000], Loss: 2.9610\n","Step[2490/5000], Loss: 2.9322\n","Step[2500/5000], Loss: 2.9403\n","Step[2510/5000], Loss: 2.9138\n","Step[2520/5000], Loss: 2.9513\n","Step[2530/5000], Loss: 2.8724\n","Step[2540/5000], Loss: 2.9157\n","Step[2550/5000], Loss: 2.9157\n","Step[2560/5000], Loss: 2.8851\n","Step[2570/5000], Loss: 2.9355\n","Step[2580/5000], Loss: 2.8209\n","Step[2590/5000], Loss: 2.9101\n","Step[2600/5000], Loss: 2.8819\n","Step[2610/5000], Loss: 2.8963\n","Step[2620/5000], Loss: 2.9044\n","Step[2630/5000], Loss: 2.9185\n","Step[2640/5000], Loss: 2.9298\n","Step[2650/5000], Loss: 2.8960\n","Step[2660/5000], Loss: 2.8990\n","Step[2670/5000], Loss: 2.8558\n","Step[2680/5000], Loss: 2.9257\n","Step[2690/5000], Loss: 2.8857\n","Step[2700/5000], Loss: 2.8888\n","Step[2710/5000], Loss: 2.8731\n","Step[2720/5000], Loss: 2.8679\n","Step[2730/5000], Loss: 2.8857\n","Step[2740/5000], Loss: 2.9440\n","Step[2750/5000], Loss: 2.8631\n","Step[2760/5000], Loss: 2.8668\n","Step[2770/5000], Loss: 2.9276\n","Step[2780/5000], Loss: 2.8919\n","Step[2790/5000], Loss: 2.8942\n","Step[2800/5000], Loss: 2.9442\n","Step[2810/5000], Loss: 2.9113\n","Step[2820/5000], Loss: 2.9035\n","Step[2830/5000], Loss: 2.8776\n","Step[2840/5000], Loss: 2.9084\n","Step[2850/5000], Loss: 2.8519\n","Step[2860/5000], Loss: 2.9008\n","Step[2870/5000], Loss: 2.8777\n","Step[2880/5000], Loss: 2.9382\n","Step[2890/5000], Loss: 2.9189\n","Step[2900/5000], Loss: 2.9159\n","Step[2910/5000], Loss: 2.8242\n","Step[2920/5000], Loss: 2.8750\n","Step[2930/5000], Loss: 2.9700\n","Step[2940/5000], Loss: 2.8944\n","Step[2950/5000], Loss: 2.8775\n","Step[2960/5000], Loss: 2.9114\n","Step[2970/5000], Loss: 2.8448\n","Step[2980/5000], Loss: 2.8788\n","Step[2990/5000], Loss: 2.9019\n","Step[3000/5000], Loss: 2.9414\n","Step[3010/5000], Loss: 2.8781\n","Step[3020/5000], Loss: 2.9060\n","Step[3030/5000], Loss: 2.9232\n","Step[3040/5000], Loss: 2.9527\n","Step[3050/5000], Loss: 2.8911\n","Step[3060/5000], Loss: 2.8818\n","Step[3070/5000], Loss: 2.7884\n","Step[3080/5000], Loss: 2.8336\n","Step[3090/5000], Loss: 2.8839\n","Step[3100/5000], Loss: 2.8824\n","Step[3110/5000], Loss: 2.8904\n","Step[3120/5000], Loss: 2.9664\n","Step[3130/5000], Loss: 2.9542\n","Step[3140/5000], Loss: 2.9119\n","Step[3150/5000], Loss: 2.8691\n","Step[3160/5000], Loss: 2.8971\n","Step[3170/5000], Loss: 2.8571\n","Step[3180/5000], Loss: 2.9475\n","Step[3190/5000], Loss: 2.8831\n","Step[3200/5000], Loss: 2.8652\n","Step[3210/5000], Loss: 2.8555\n","Step[3220/5000], Loss: 2.9293\n","Step[3230/5000], Loss: 2.9357\n","Step[3240/5000], Loss: 2.9058\n","Step[3250/5000], Loss: 2.8632\n","Step[3260/5000], Loss: 2.8820\n","Step[3270/5000], Loss: 2.8825\n","Step[3280/5000], Loss: 2.9313\n","Step[3290/5000], Loss: 2.8678\n","Step[3300/5000], Loss: 2.9245\n","Step[3310/5000], Loss: 2.8397\n","Step[3320/5000], Loss: 2.9065\n","Step[3330/5000], Loss: 2.7604\n","Step[3340/5000], Loss: 2.8655\n","Step[3350/5000], Loss: 2.8060\n","Step[3360/5000], Loss: 2.9366\n","Step[3370/5000], Loss: 2.8802\n","Step[3380/5000], Loss: 2.9107\n","Step[3390/5000], Loss: 2.9023\n","Step[3400/5000], Loss: 2.8120\n","Step[3410/5000], Loss: 2.8924\n","Step[3420/5000], Loss: 2.8944\n","Step[3430/5000], Loss: 2.8694\n","Step[3440/5000], Loss: 2.8845\n","Step[3450/5000], Loss: 2.8954\n","Step[3460/5000], Loss: 2.8195\n","Step[3470/5000], Loss: 2.8487\n","Step[3480/5000], Loss: 2.9214\n","Step[3490/5000], Loss: 2.9115\n","Step[3500/5000], Loss: 2.8261\n","Step[3510/5000], Loss: 2.8455\n","Step[3520/5000], Loss: 2.8448\n","Step[3530/5000], Loss: 2.8796\n","Step[3540/5000], Loss: 2.8824\n","Step[3550/5000], Loss: 2.8719\n","Step[3560/5000], Loss: 2.8293\n","Step[3570/5000], Loss: 2.9657\n","Step[3580/5000], Loss: 2.8904\n","Step[3590/5000], Loss: 2.9033\n","Step[3600/5000], Loss: 2.8197\n","Step[3610/5000], Loss: 2.8694\n","Step[3620/5000], Loss: 2.8775\n","Step[3630/5000], Loss: 2.8575\n","Step[3640/5000], Loss: 2.8720\n","Step[3650/5000], Loss: 2.9458\n","Step[3660/5000], Loss: 2.7312\n","Step[3670/5000], Loss: 2.8761\n","Step[3680/5000], Loss: 2.8955\n","Step[3690/5000], Loss: 2.9365\n","Step[3700/5000], Loss: 2.8516\n","Step[3710/5000], Loss: 2.8813\n","Step[3720/5000], Loss: 2.8808\n","Step[3730/5000], Loss: 2.8363\n","Step[3740/5000], Loss: 2.7776\n","Step[3750/5000], Loss: 2.8877\n","Step[3760/5000], Loss: 2.7592\n","Step[3770/5000], Loss: 2.8900\n","Step[3780/5000], Loss: 2.9225\n","Step[3790/5000], Loss: 2.8291\n","Step[3800/5000], Loss: 2.8206\n","Step[3810/5000], Loss: 2.8620\n","Step[3820/5000], Loss: 2.9125\n","Step[3830/5000], Loss: 2.8039\n","Step[3840/5000], Loss: 2.8440\n","Step[3850/5000], Loss: 2.8602\n","Step[3860/5000], Loss: 2.8434\n","Step[3870/5000], Loss: 2.9221\n","Step[3880/5000], Loss: 2.8300\n","Step[3890/5000], Loss: 2.8773\n","Step[3900/5000], Loss: 2.8221\n","Step[3910/5000], Loss: 2.8159\n","Step[3920/5000], Loss: 2.8828\n","Step[3930/5000], Loss: 2.9123\n","Step[3940/5000], Loss: 2.8407\n","Step[3950/5000], Loss: 2.8148\n","Step[3960/5000], Loss: 2.8015\n","Step[3970/5000], Loss: 2.8593\n","Step[3980/5000], Loss: 2.8903\n","Step[3990/5000], Loss: 2.8506\n","Step[4000/5000], Loss: 2.9145\n","Step[4010/5000], Loss: 2.7676\n","Step[4020/5000], Loss: 2.8486\n","Step[4030/5000], Loss: 2.8298\n","Step[4040/5000], Loss: 2.8799\n","Step[4050/5000], Loss: 2.9406\n","Step[4060/5000], Loss: 2.8703\n","Step[4070/5000], Loss: 2.8268\n","Step[4080/5000], Loss: 2.8779\n","Step[4090/5000], Loss: 2.8975\n","Step[4100/5000], Loss: 2.8804\n","Step[4110/5000], Loss: 2.8254\n","Step[4120/5000], Loss: 2.8930\n","Step[4130/5000], Loss: 2.8142\n","Step[4140/5000], Loss: 2.8940\n","Step[4150/5000], Loss: 2.8976\n","Step[4160/5000], Loss: 2.8608\n","Step[4170/5000], Loss: 2.8811\n","Step[4180/5000], Loss: 2.8525\n","Step[4190/5000], Loss: 2.8343\n","Step[4200/5000], Loss: 2.7947\n","Step[4210/5000], Loss: 2.8559\n","Step[4220/5000], Loss: 2.8632\n","Step[4230/5000], Loss: 2.8308\n","Step[4240/5000], Loss: 2.8266\n","Step[4250/5000], Loss: 2.8614\n","Step[4260/5000], Loss: 2.8944\n","Step[4270/5000], Loss: 2.9202\n","Step[4280/5000], Loss: 2.7521\n","Step[4290/5000], Loss: 2.7773\n","Step[4300/5000], Loss: 2.8077\n","Step[4310/5000], Loss: 2.8856\n","Step[4320/5000], Loss: 2.8311\n","Step[4330/5000], Loss: 2.8962\n","Step[4340/5000], Loss: 2.8791\n","Step[4350/5000], Loss: 2.8508\n","Step[4360/5000], Loss: 2.8407\n","Step[4370/5000], Loss: 2.8207\n","Step[4380/5000], Loss: 2.8655\n","Step[4390/5000], Loss: 2.7906\n","Step[4400/5000], Loss: 2.7968\n","Step[4410/5000], Loss: 2.8368\n","Step[4420/5000], Loss: 2.8376\n","Step[4430/5000], Loss: 2.8496\n","Step[4440/5000], Loss: 2.9359\n","Step[4450/5000], Loss: 2.8563\n","Step[4460/5000], Loss: 2.8291\n","Step[4470/5000], Loss: 2.7683\n","Step[4480/5000], Loss: 2.7831\n","Step[4490/5000], Loss: 2.8726\n","Step[4500/5000], Loss: 2.8794\n","Step[4510/5000], Loss: 2.8979\n","Step[4520/5000], Loss: 2.7972\n","Step[4530/5000], Loss: 2.7966\n","Step[4540/5000], Loss: 2.7859\n","Step[4550/5000], Loss: 2.8518\n","Step[4560/5000], Loss: 2.8965\n","Step[4570/5000], Loss: 2.8606\n","Step[4580/5000], Loss: 2.8420\n","Step[4590/5000], Loss: 2.8713\n","Step[4600/5000], Loss: 2.8483\n","Step[4610/5000], Loss: 2.8267\n","Step[4620/5000], Loss: 2.8461\n","Step[4630/5000], Loss: 2.8485\n","Step[4640/5000], Loss: 2.7669\n","Step[4650/5000], Loss: 2.8290\n","Step[4660/5000], Loss: 2.9004\n","Step[4670/5000], Loss: 2.8656\n","Step[4680/5000], Loss: 2.8569\n","Step[4690/5000], Loss: 2.8428\n","Step[4700/5000], Loss: 2.8501\n","Step[4710/5000], Loss: 2.7938\n","Step[4720/5000], Loss: 2.8564\n","Step[4730/5000], Loss: 2.8357\n","Step[4740/5000], Loss: 2.8096\n","Step[4750/5000], Loss: 2.7944\n","Step[4760/5000], Loss: 2.8136\n","Step[4770/5000], Loss: 2.8053\n","Step[4780/5000], Loss: 2.8587\n","Step[4790/5000], Loss: 2.8564\n","Step[4800/5000], Loss: 2.8105\n","Step[4810/5000], Loss: 2.8492\n","Step[4820/5000], Loss: 2.8219\n","Step[4830/5000], Loss: 2.8317\n","Step[4840/5000], Loss: 2.7809\n","Step[4850/5000], Loss: 2.8756\n","Step[4860/5000], Loss: 2.8810\n","Step[4870/5000], Loss: 2.8528\n","Step[4880/5000], Loss: 2.8163\n","Step[4890/5000], Loss: 2.8504\n","Step[4900/5000], Loss: 2.8187\n","Step[4910/5000], Loss: 2.7948\n","Step[4920/5000], Loss: 2.8233\n","Step[4930/5000], Loss: 2.8307\n","Step[4940/5000], Loss: 2.8500\n","Step[4950/5000], Loss: 2.8385\n","Step[4960/5000], Loss: 2.8775\n","Step[4970/5000], Loss: 2.8119\n","Step[4980/5000], Loss: 2.7943\n","Step[4990/5000], Loss: 2.8028\n","Step[5000/5000], Loss: 2.8161\n"]}],"source":["train()"]},{"cell_type":"markdown","metadata":{},"source":["Testing model on sample prompts"]},{"cell_type":"code","execution_count":233,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T02:43:42.343009Z","iopub.status.busy":"2024-02-27T02:43:42.342205Z","iopub.status.idle":"2024-02-27T02:44:02.414212Z","shell.execute_reply":"2024-02-27T02:44:02.413314Z","shell.execute_reply.started":"2024-02-27T02:43:42.342976Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","                    How To Download With No Disk Space\n","                \n","Take tell themselves—but police themselves\n","With military course, and the course\n","It would not exit them.\n"," \n","Not a good man, not a good man,\n","Each one with a career like a bee\n","Above my feet, links his left leg of legs.\n","You aspire to what’s enough\n","One by one. Each given a bouquet\n","And say goodbye.\n"," \n","The way they got them must by, both made them from\n","The forgotten except for me.\n","Then the kid didn’t exactly, ‘You,’&\n","That you once said ‘Warning.’ No, but I begun\n","On the Rockyak, as the Isle of Obient Blood Market.\n","When it took to the square station\n","They climbed the restaurant. When it had established:   \n","‘What will we have to want?’ He got\n","‘Spoken! If the Past is dear.’\n","‘Though it was I wouldn’t have to say, ‘I can’t.\n","You would know which to watch and\n","The Aristotle, if rather you   \n","Symphonies whether they,\n","Feel up one cabinet or fish or to please or to blame\n","A way: ‘Pop or Gratural poet!’)\n","I have no capbacks, no trees,\n","With lines, grass, or blood, and then read\n","The Blacks of the Bottle. ‘The boat\n","Of the Kumaroon’s Keptember makes\n","Sniffer through a skull, and strike,\n","Warm benches, north, north, north, north,\n","Not wrong nor quite prefered.’\n","V\n","When it fell, the fog had dramatically drowned,\n","Sniffling, in a dark room of traffic\n","Traffic boats; some of the third moments, and had turned\n","In a space, no matter where the boy had killed\n","And had locked his hands around it,\n","And then lay straight down on an enormous hair\n","Can light up a passionate voice\n","Strife at the moment. The Night slow\n"," \n","Ladders his eye—there so still,\n","I threadbare his talk and ladder he was\n","Already moving those crests, flooding themselves\n","Passing across careening.\n"," \n","So resentful he never knew for you\n","And for all the changing of his life.\n"," \n","The listener\n","Save it his flower with alice.\n"," \n","We began our birthday nights\n","To hunt with a thread of night\n","Our living words.\n"," \n","We were silence with ease.\n"," \n"," \n"," \n","Warhol, felled freight on thir minds,\n","In the river of our new suit.\n"," \n"," \n"," \n"," \n",".\n"," \n"," \n","He beathed the runs and journey light\n","Falling through runners and doesn’t exist.\n","Give praise. A running knife shouting out of his hand.\n","This boy and a haiku\n","As if all the time were flown around him\n"," \n","He sat down and found him.\n","Everyone else would cook and grease.\n"," \n"," \n"," \n"," \n","The ‘Doctor’ run away if all those of our digits.\n","\n"]}],"source":["generatePoem(\"How To Download With No Disk Space\")"]},{"cell_type":"code","execution_count":201,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T02:02:08.846560Z","iopub.status.busy":"2024-02-27T02:02:08.845615Z","iopub.status.idle":"2024-02-27T02:03:15.282502Z","shell.execute_reply":"2024-02-27T02:03:15.281613Z","shell.execute_reply.started":"2024-02-27T02:02:08.846525Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","                    The Boy Who Lived, Come To Die!\n","                \n","“So we escaped.”\n","So oyster orphans shooted\n","and who came to climb black\n","and a crush of rabid pods, mother\n","from one poodle to the easel\n","Pulsing a dream in the next b.dad drew\n","more wedding, then yelled Rhetoric, my grandfather’s back\n","and when he sat down on Mesopotamis decided\n","I got tired in this bag\n","as he told his mother knew caught\n","so he said he. But just enough.\n","Still, he started, his body laughed\n","and said he waited for his children.\n","I announced him'd get them.\n","Dad said he was a human son,\n","he was a class, a woman, a small,\n","a prisoner and a wave clucking\n","in the front of a naked man,\n","giving a brand new tarp,\n","and\n","talk to Smith in his table—\n","no: lift up my legs\n","a safe leaf and fall.\n","Space was a big mouse.\n","They found the world\n","and the rest were alive\n","beside me. The boy\n","wrang to engirt boar.\n","I have seen those who\n","were at the three farmer’s table\n","in the back of a car\n","and, so as to California, Whitman, on the Street Margin,\n","or large with large planes\n","where they could stay awfully\n","among us? Curtains,\n","our shoes, arranges, home, west, roast.\n","You know one. You see the large smell\n","of screeching hair, a thousand girls\n","snow blowing at the crowd, or\n","the others’ garden of glittering.\n","          4\n","As the earth, music became\n","across the street, the distant\n","lines of a face familiar with large castle\n","smeared as they found a sheet even on the\n","tornado as the sky dark sucked\n","from the elements of the women\n","in the dark, a certain literature.\n","         No one seemed in me—anyone else.\n","Was it that I forgot—\n"," \n","like you I recognized, or the little children\n","falling in to cry. Bermudgettes of trees\n","arrived and crammed cattle to braid?\n","Impossible to find myself\n","slaking out of bursting air—\n"," \n","Always I watched you, watched you stayed in the warm sun\n"," \n","of a Chinese place. And one of you twinkled in the red flannel\n","left on a stalk in the air, cement the engine rang\n"," \n","broad, banana occurred to spatior scalding\n","above the deep, the soul that remained, but didn’t\n","advance K, but couldn’t be so much space again\n","understand itself, swipes us, closes us through everything? Was\n","you’d have to name all you be adored? It’s as if\n","it arrives before you were not perhaps that being red, or bored\n","your sweep into space, and the idle pearl or two\n","there was a rock covering and a sheet,\n","so it was younger and my own\n","sheriff, satin-googled. To keep food\n","in my breasts, take the snake, each pair of lullaby\n","sheer to the thorn of his thumbs. Old hands\n","to fold my view. Thought man was wearing a gleaner,\n","quilling to a job-not love. And I believe my mother\n","there was a coyote who put the man down.\n","I’ll always wait again. Don’t sleep.\n"," \n"," \n","This was my name.\n"," \n","She shook my remembers as if me to return back home,\n","baby, teeming to scraw indite.\n","I did for return to the living.\n","I did not lute then.\n"," \n"," \n","When I lifted in my hand, the third\n","still casually chained.\n"," \n"," \n"," \n","V\n"," \n","I heard the lift of you as I moved\n","which stores me still moreoy;\n","than my magic oddness denying\n","as I went.\n"," \n"," \n"," \n"," \n"," \n","3.\n","I saw the stars, so I could see\n","his stars—a dark coralist of water\n","so that in mighty eyes,\n","the young women without waste,\n","they were maré, not imagining\n","the heavy in the wind, kneehüg—\n"," \n","hémos; an accidentist, sad,\n","following the watery angel, sadly\n","that goescape, sadly, and shooting\n","the pianos one by one,\n","the currents beauty shoe, the lacquered\n"," \n","the frame of habitual conversation\n","left here to me, they habit, speculating\n","phenomenon. The noise of “A western linkof   English” commentated,\n"," \n","for those valivating effects that from here\n","the promise of   century, you will want to\n","see to see me, it’s possible that I talked\n","again. Those of   breakfast  those who were\n","almost    ...   \n"," George Helen\n","Cherry Cream Galbe Galiley wear        ...   \n","They are being young again\n","Jealous Buenos’s met the their\n","hapes..   Jealous Rain\n","Asked \"Only Ore the...”\n","                   “Do I know just about\n","They are merely discriminate, and I are what they meant\n","to answer for they.”\n","They took them down\n","the place of their later faces\n","Where aslaw\n","had left, said perfectly,\n","“What about this moment?”\n","(Each moment trying to be\n","Others had to do this—\n","Grandma undertowed his supervision of the tables\n","caves), digits, grandmas\n","(a lawyer . . . not too much heroes)\n","in a long consent heroic teachery\n","(a heroine, a seaone-poon.)\n","and so the raiders skip themselves in the parti-gap,\n","grace barrel and gray and high,\n","good frogdame of blood. I assume\n","in the snow, from the house—\n","a chasm of ice, file, and bell-cut,\n","I hear the whirring gleam at the Ship Farris-eating,\n","Ship, eating the great stars,\n","    flung in and out and meet,\n","take the dark path, turn in the noon and the house\n","in the danger, glint in\n","between the stars and the ochre.\n","The wind flickers deeper, heavy chamber.\n","The battle trees and cools cover\n","and sea cover the air over it,\n","and, a raw log and the leaves\n","look out their hearts, their faces\n","are supplely moving forward from the heat.\n"," \n","Did you understand the valleys without even tomorrow?\n","6 New York, Pucca, Trumano, Ihuedo.\n","Her name is about the newspaper, clearly, without a block\n","adorned with its jaws for nothing. On a shore\n","gives coming in to ground to make us hunger.\n","People give important deconstructions like, so that\n","types bombed of epic, held tight across heaven,\n","affronted by dipped frown with sunlight and theyade.\n","Love's positive shape I forecast among many poles\n","eclipses? we make no generation in it.\n","When thirteen three long will they do in it.\n","I'm integrating the mystic arts of it.\n","In 1969 million dollars and women, harp-heads with their weaknesses;\n","my cars go down over the silent land again, keeping them,\n","their naked documentary growing, bordering\n","into my wolves. I stand in exile\n","that wolves with no haze,\n","their nouns and their actions' eyes,\n","and twitter indicates them,\n","who can't tell a mountain's pleasure\n","or better without suggest,\n","that mountains ranged in grief,\n","and felt in sunlight, waking.\n","I stare to see no curse\n","that announced the surface.\n","And I dreamt I thought\n","that her pulse was too clear\n","and too few.\n","It was so clear but it was good for that, upturning\n","until I took off my hands.\n","The purl beef, I took up\n","the lizard, to keep my hands on my own.\n","We chased her right at the other\n","extinct like an undanimated confident.\n","I imagined so unafraid. Loud summer\n","dazed my voice from my eyes.\n"," \n","“That after I turned the glass.”\n","I felt spinning a limnifer. And so, inside.\n","The though I was ten years later, an argument\n","carried something other than a hand-an-hand, a screen.\n","I begged at the backdoor of my dreams, I prayed\n","to remember what I saw, happens,\n","walking through the window, until I saw you felt\n","talking. The brain carried me from a doorway window.\n","I read that well, I was never very well told\n","that given it to you, and we had some ration,\n","and we were surrounded much real. You were arterial\n","I find a burrounded face, back and forth\n","car hurled it with fire, foxes wrapped in locked feet,\n","and I kept it in doors, penned them under a river.\n","Ten years ago in back, the world was a question\n","in that made-up world, I wanted to spit out.\n","Why? I said so. My weapons?\n","Everything that was left of something. I said the liver\n","frothing. I wasn’t so eventually finally enough.\n","The question had events, or I don’t absorb it.\n","Does that not just the poet?\n","Does this place keep that the invalid isn’t affected?\n","I say I would be confetted.\n","And here I hear the safe calligraphies shut.\n","And here I will see my veins and the paper pipe move.\n","\n"]}],"source":["generatePoem(\"The Boy Who Lived, Come To Die!\") # take 1"]},{"cell_type":"code","execution_count":193,"metadata":{"execution":{"iopub.execute_input":"2024-02-27T01:16:24.654694Z","iopub.status.busy":"2024-02-27T01:16:24.654312Z","iopub.status.idle":"2024-02-27T01:17:06.009688Z","shell.execute_reply":"2024-02-27T01:17:06.008718Z","shell.execute_reply.started":"2024-02-27T01:16:24.654647Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","                    The Boy Who Lived, Come To Die!\n","                \n","\n","\n","\n","1\n","—Chapter 26\n","\n","\n","\n","She was bloody, the lover never called,\n","And all of these the broken houses as she lived, in the garden,\n","She mutted, with wounded left, evicted slits, as she walked through\n","\n","Whenever to walk the barrel or marvelous little multiply\n","\n","I went tofore with the idea. The Fumes clang like to Billity\n","\n","shiven pecks Billow, and Joseph said: 'Harvesters, quickens\n","\n","New monads on Joe's face; this she be thrived\n","\n","As everyday belongs for her to celebrate with any other,\n","Even Louisia she cut from Fortono Samo is holy\n","\n","(I called of Samo’s Vietnamono Physic)\n","\n","My location in the way Labora writes and writhes writings.•\n","\n","O saint, friendly homer ritual?\n","\n","Begun\n","\n"," \n","\n","Haitella •\n","\n","I was writing down the early\n","\n","• •\n","\n"," \n","\n","What's not sure if the whole •\n","\n","Yours • mine •\n","\n","when watching\n","\n","your kids\n","\n","you are stoping,\n","\n","I hope\n","\n","if we’re yet getting\n","\n","because you left to be\n","\n","a hygriphic melody\n","\n","every day and sun\n","\n","you marry •\n","\n"," \n","\n","dinoxy memory of   language I have been doing\n","\n","reciting me me. His pretended notes\n","\n","has been continued for nothing.\n","\n","Can I ever have\n","\n","leaves, such a perviling\n","\n","splits sour into housetricles\n","\n","of the dreams that Srbtle now noting\n","\n","can be families but with endings\n","\n","of the names like snakes. The viscera\n","\n","of Goya’s question rattles now\n","\n","then at the leastly drought end\n","\n","matters American\n","\n","& Names Hambor\n","\n","Goya rule stay alongside\n","\n","the city into suicide\n","\n","all night the world becomes a rich\n","\n","the old story contrary\n","\n","for the widening;\n","\n","God said\n","\n","dad he\n","\n","about datis enough\n","\n","proudo; he didn’t want\n","\n","along he just could see who’s\n","\n","sunk into a provoca. The arc of time\n","\n","he said for\n","\n","your means a still\n","\n","like this: no egg; no.\n","\n","They slow to the last year\n","\n","shaping around the names\n","\n","when the transis is extravagant with gold.\n","\n","The desolation is public\n","\n","ready vowed with all continues, staby\n","\n","An angel shows her acres would pay.\n","\n","Teachings\n","\n","a girl doesn’t even stop\n","\n","the Hondusoreon. Frederias then\n","\n","after the power through.\n","\n","Friendsky schools have their reorts from\n","\n","to contain the snot\n","\n","homeland kills and kills. The Puritos\n","\n","because when the quills are open\n","\n","as itself you can see something\n","\n","wrong to see what is different. Also age to his place\n","\n","from a Cavalier or at Portuguese orphan\n","\n","you can’t see stammed on things\n","\n","and recordion put down on Sarah. Don’t say Byrd\n","\n","toward more city yet. What is their seraphim, stabbath, waver, leave to searz\n","\n"," \n","\n","pitiate spaces. On another storm. Don’t try to ashes that’s how this\n","\n","black skin has been dropped out. Don’t ask their vain\n","\n","to be known. They can't cool the swan, the men in a good pond. Here the readers have just no answer.\n","And Is it I am on distant things a great Sput fish?\n","\n"," \n","\n","They are too silent, because of this point that clips everywhere and now the continues possible\n","\n","parsonal and show up their fathers in the eyelids.\n","\n","His street is a little for relationship to the army of this pieces. And when he is standing at the farm red John Ferate’s and out,\n","\n","and here is something for the person who sits back from the reply to miss him\n","\n","the where a child and stands on a seat\n","\n","will gather from the new valley of the idea\n","\n","and all along to the people and the people who want to carry\n","\n","inside an air corner of the world and want to put your greatest government, one fathomed to bring.\n","\n","That’s how old?\n","\n","“Like a lesson?”\n","\n","“Moving upon the lifeline.\n","\n","“A sift and thought for what,” he said.\n","\n","“What’s one went home?\n","\n","“No kind,” after having left me one night!”\n","\n","“No one ever had liked me for another time.”\n","\n","“And what?” “That you mean to fight in that nothing places\n","\n","To his underqueath:\n","\n","” “What will you have to do?” she says, “How\n","\n","” “That what is into my hand?” “Is this the composure?”\n","\n","“Do you lose,”             and fork in the middle of   you’rehold.\n","\n"," \n","\n","“If you look out what I love your journey would have\n","\n","Well,” she says. And “John Reilings come,”\n","\n","What is Socio.” “What is it?”\n","\n","Well, I’m instrumented now,\n","\n","what was to grow, what I’m going to make how,\n","what I’m smooth on. I’m sore and I’m not\n","\n","human, but too little.”\n"," \n","\n","\n","Snow clings, withers, withers:\n","\n","Lank stares, colors claw, press\n","\n","lost lead one’s!”\n","\n","Yellow fades up her waitress\n","\n","lost their bellies like a pipe,\n","\n","art in a shark, the world\n","\n","comes alive for their roosts\n","\n","and echoes of wine\n","\n","in a composition. You know\n","\n"," \n","\n","But that’s what you feel as if\n","\n","to reduce a flock of fresher\n","\n","yours, the paint itself\n","\n","the world is already carefully\n","\n","to stand. For it is supposed\n","\n","to read time the ruial proper think you had\n","\n","fit the poems,\n","\n","stomp the most simple if the letter\n","\n","you know the story of sky\n","\n","was going quickly with yours\n","\n","which mimic minds a sky\n","\n","affuler. Lost that doesn’t think\n","\n","why you know.\n","\n"]}],"source":["generatePoem(\"The Boy Who Lived, Come To Die!\") # take 2"]},{"cell_type":"markdown","metadata":{},"source":["The stuff below can be ignored but I left it anyways because it gives another example of poem generation"]},{"cell_type":"markdown","metadata":{},"source":["<h1>Generation Record:</h1>\n","- model with no special tokens, loss: 3.5671, dropout 0.2, 2k training steps, 3e-4 constant learning rate, context = \"How many more times are needed?\":\n","<br/>\"And at the st fades alongs,   \n","Most the trees read God’s venom.   \n","He blazes its dread as to be read to’s grove?   \n","Gainting a new dust’s corpselve hot feet until he is seen   \n","On the goon place were darked across The Clay,\n","Nor blow through a Hw settled field.\n","Himmering cries his sweat.\n","With news were slopes into the scream,   \n","With each furnace in sand in the news and candle!\n","Sept to the best of the air which in Sandring   \n","Her night declines in the Languis of Columbus,   \n","As the rock is a light of collectors\n","And by the Trymina out at the Winden,\n","Gream the State, by a cottage   \n","Her linger and the sight of itself like the onyard—   \n","Present will read for the tray—   \n","The distance inward thoughtres,“The diaster, white peace-echnight\n","Change with one shall glorify in the preserve?\n","Oh, I will never know and that I had a long,   \n","Cless you “Your man’s name!’ converse! Pone’s nose his   \n","And I Come to schoolm hell ate in these humans,\n","Nothing but this struther shouldn’t home,\n","Bad may go on to wish those shoes;   \n","No a snarch-set row. He is like a quiet rattle   \n","Sorrowing always feel fit the waves.\n","I nought them horse. He’s still wet.\n","Out of us teach history to\n","We are the weather's snow. The father says,   \n","Even another moving, it is to follow that.   \n","The deal can beeside them to be complain.   \n","You must move that that we should see him look   \n","My stand will keep me around   \n","The waves and weep,   \n","Their graves are of our Death   \n","In their show and does billbows me ambiable,   \n","And we have those dominion of the Names   \n","Even snows, hair visited his side beam,   \n","The imagine wife of our Field. And he them, his specklife   \n","Her suddenness is the Armisto in the God’s Subau, let his mother's same chose   \n","Death and her and his own song—my priest?   \n","Ne hope? He must consider him death, we will   \n","Over that each graceful on the Life of his cannot see. Be always didn’t helphe undiminently so.\n","Decrets them as coming. Had he did she taught us?\n","He said with him and he wants sank, he went on with all,\n","See his shrivabor’s nodding blood, but he could have no mein.   \n","He’d like him told us what it’s repeat,   \n","You never was soon again. See what they should   \n","To ever go with him, therefore the sum9thwhile I knew,\n","When I shall not be saved from the cloyest man was strength,   \n","Are his capture and he was clear;\n","Which were my power he had said,   \n","What learned her from ‘But I’m knew\n","The Barret Earth Obs’: “Here, a queen!” Dad,\n","“Who creeps the highway where vere of evening!”   \n","Paradise speckle, and with a body to believe of\n","Lady to a self, to sleep, working but there,   \n","Too good, o\""]},{"cell_type":"code","execution_count":97,"metadata":{"execution":{"iopub.execute_input":"2024-02-26T23:41:04.486941Z","iopub.status.busy":"2024-02-26T23:41:04.486534Z","iopub.status.idle":"2024-02-26T23:41:27.548301Z","shell.execute_reply":"2024-02-26T23:41:27.547326Z","shell.execute_reply.started":"2024-02-26T23:41:04.486908Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["How many more times are needed?   \n","And at the st fades alongs,   \n","Most the trees read God’s venom.   \n","He blazes its dread as to be read to’s grove?   \n","Gainting a new dust’s corpselve hot feet until he is seen   \n","On the goon place were darked across The Clay,\n","Nor blow through a Hw settled field.\n","Himmering cries his sweat.\n","With news were slopes into the scream,   \n","With each furnace in sand in the news and candle!\n","Sept to the best of the air which in Sandring   \n","Her night declines in the Languis of Columbus,   \n","As the rock is a light of collectors\n","And by the Trymina out at the Winden,\n","Gream the State, by a cottage   \n","Her linger and the sight of itself like the onyard—   \n","Present will read for the tray—   \n","The distance inward thoughtres,“The diaster, white peace-echnight\n","Change with one shall glorify in the preserve?\n","Oh, I will never know and that I had a long,   \n","Cless you “Your man’s name!’ converse! Pone’s nose his   \n","And I Come to schoolm hell ate in these humans,\n","Nothing but this struther shouldn’t home,\n","Bad may go on to wish those shoes;   \n","No a snarch-set row. He is like a quiet rattle   \n","Sorrowing always feel fit the waves.\n","I nought them horse. He’s still wet.\n","Out of us teach history to\n","We are the weather's snow. The father says,   \n","Even another moving, it is to follow that.   \n","The deal can beeside them to be complain.   \n","You must move that that we should see him look   \n","My stand will keep me around   \n","The waves and weep,   \n","Their graves are of our Death   \n","In their show and does billbows me ambiable,   \n","And we have those dominion of the Names   \n","Even snows, hair visited his side beam,   \n","The imagine wife of our Field. And he them, his specklife   \n","Her suddenness is the Armisto in the God’s Subau, let his mother's same chose   \n","Death and her and his own song—my priest?   \n","Ne hope? He must consider him death, we will   \n","Over that each graceful on the Life of his cannot see. Be always didn’t helphe undiminently so.\n","Decrets them as coming. Had he did she taught us?\n","He said with him and he wants sank, he went on with all,\n","See his shrivabor’s nodding blood, but he could have no mein.   \n","He’d like him told us what it’s repeat,   \n","You never was soon again. See what they should   \n","To ever go with him, therefore the sum9thwhile I knew,\n","When I shall not be saved from the cloyest man was strength,   \n","Are his capture and he was clear;\n","Which were my power he had said,   \n","What learned her from ‘But I’m knew\n","The Barret Earth Obs’: “Here, a queen!” Dad,\n","“Who creeps the highway where vere of evening!”   \n","Paradise speckle, and with a body to believe of\n","Lady to a self, to sleep, working but there,   \n","Too good, o\n"]}],"source":["generatePoem(\"How many more times are needed?\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":236282,"sourceId":502516,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
